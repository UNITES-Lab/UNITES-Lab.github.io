<!DOCTYPE html>
<html lang="en">

<head>
    <title>VITA</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


    <link href="https://fonts.googleapis.com/css?family=B612+Mono|Cabin:400,700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="fonts/icomoon/style.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">
    <link href="css/jquery.mb.YTPlayer.min.css" media="all" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="css/style.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->


</head>

<body data-spy="scroll" data-target=".site-navbar-target" data-offset="300">

<div class="site-wrap">

    <div class="site-mobile-menu site-navbar-target">
        <div class="site-mobile-menu-header">
            <div class="site-mobile-menu-close mt-3">
                <span class="icon-close2 js-menu-toggle"></span>
            </div>
        </div>
        <div class="site-mobile-menu-body"></div>
    </div>

    <div class="header-top">
        <div class="container" style="padding:20px">
            <div class="row align-items-center">
                <!-- <div class="col-12 col-lg-6 d-flex"> -->
                    <img src="./logo.png" width="15%">
                    <a class="ml-auto site-logo">
                       
                             &nbsp&nbsp<b style="color: rgb(71, 71, 71)">V</b>isual <b style="color: rgb(71, 71, 71)">I</b>nformatics Group @ University of <b style="color: rgb(71, 71, 71)">T</b>exas at <b style="color: rgb(71, 71, 71)">A</b>ustin
                    </a>
                    <a href="#"
                       class="ml-auto d-inline-block d-lg-none site-menu-toggle js-menu-toggle text-black"><span
                            class="icon-menu h3"></span></a>

                <!-- </div> -->
                <!-- <div class="col-12 col-lg-6 ml-auto d-flex">
                    <div class="ml-md-auto top-social d-none d-lg-inline-block">
                        <a href="#" class="d-inline-block p-3"> </a>
                        <a href="#" class="d-inline-block p-3"> </a>
                        <a href="#" class="d-inline-block p-3"> </a>
                    </div>

                </div> -->
                <!--          <div class="col-6 d-block d-lg-none text-right">-->

            </div>
        </div>
    </div>

    
    <div class="site-navbar py-2 js-sticky-header site-navbar-target d-none pl-0 d-lg-block" role="banner">

        <div class="container" style="padding-right=10%">
            <div class="d-flex align-items-right">
                <!-- <div class="mr-auto">
                    <a href="index.html">
                       <img src="./logo.png" width="10%"/>
                             &nbsp&nbsp<b>V</b>isual <b>I</b>nformatics Group @ University of <b>T</b>exas at <b>A</b>ustin
                    </a>
                </div> -->
                <div class="ml-auto">
                    <nav class="site-navigation position-relative text-right" role="navigation">
                        <ul class="site-menu main-menu js-clone-nav mr-auto d-none pl-0 d-lg-block">
                            <li class="active">
                                <a href="index.html" class="nav-link text-right">Home</a>
                            </li>
                            <li>
                                <a href="research.html" class="nav-link text-left">PI & Research</a>
                            </li>
                            <li>
                                <a href="publication.html" class="nav-link text-left">Publication</a>
                            </li>
                            <li>
                                <a href="group.html" class="nav-link text-left">Group</a>
                            </li>
                            <li>
                                <a href="resource.html" class="nav-link text-left">Resource</a>
                            </li>
                            <li>
                                <a href="prospective_students.html" class="nav-link text-left">Opening</a>
                            </li>
                            <!-- <li class="nav-item dropdown">
                                              <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                   Challenge
                                 </a>
                                          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                           <a class="dropdown-item" href="challenge1.html">Tiny Object Detection Challenge</a>
                                           <a class="dropdown-item" href="challenge2.html">Image Restoration for UDC Challenge</a>
                                         </div>
                            </li>
                            <li>
                                <a href="callforpapers.html" class="nav-link text-left">Call for Papers</a>
                            </li>

                            <li>
                                <a href="speakers.html" class="nav-link text-left">Invited Speakers</a>
                            </li>

                            <li class="nav-item dropdown">
                                <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown"
                                   role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                    Previous
                                </a>
                                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                    <a class="dropdown-item" href="https://yuqian2.wixsite.com/forlq">RLQ'19</a>
                                </div>
                            </li> -->
                        </ul>
                    </nav>

                </div>

            </div>
        </div>

    </div>

</div>



<div class="site-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-12">

                
                <div class="section-title" style="margin-bottom:20px">
                    <h2>About PI</h2>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        




                        <p>Professor Zhangyang “Atlas” Wang <a href="https://scholar.google.com/citations?user=pxFyKAIAAAAJ&hl=en">[Google Scholar]</a> is a tenured Associate Professor and holds the Temple Foundation Endowed Faculty Fellowship #7, in the Chandra Family Department of Electrical and Computer Engineering at The University of Texas at Austin. He is also a faculty member of UT Computer Science <a href="https://csrankings.org/"> [CSRankings]</a>, and the Oden Institute CSEM program. <b style="color:rgb(71, 71, 71)">Since May 2024, Dr. Wang has been on leave from UT Austin to serve as the full-time Research Director for XTX Markets</b>, heading the newly established <a href="https://www.xtxmarkets.com/career/xty-labs-ai-residency/">AI Lab</a> in New York City. In this role, he leads groundbreaking efforts at the intersection of algorithmic trading and deep learning, driving the development of robust, scalable AI algorithms to extract predictive insights from massive datasets.</p>


                       <p>Previously, he was the Jack Kilby/Texas Instruments Endowed Assistant Professor in the same department from 2020 to 2023; and an Assistant Professor of Computer Science and Engineering at Texas A&M University from 2017 to 2020. Alongside his academic career, he has also explored multiple exciting opportunities in the industry. He was a visiting scholar at Amazon Search from 2021 to 2022, leveraging geometric deep learning for recommendation systems. Later, he took on the (part-time) role of Director of AI Research & Technology for <a href="https://picsart.com/">Picsart</a> from 2022 to 2024, where he led the company’s ambitious initiative in video generative AI. He earned his Ph.D. in Electrical and Computer Engineering from UIUC in 2016, under the guidance of Professor Thomas S. Huang, and his B.E. in EEIS from USTC in 2012.</p>

                        <p>Prof. Wang has broad research interests spanning from the theory to the application aspects of machine learning (ML) and optimization. Currently, his research passion centers on developing the theoretical and algorithmic foundations of <b style="color:rgb(71, 71, 71)">generative AI</b> and <b style="color:rgb(71, 71, 71)">neurosymbolic AI</b>. He emphasizes <b style="color:rgb(71, 71, 71)">low-dimensional, modular representations</b> that enable efficient and reliable learning in overparameterized model spaces while bridging the gap to symbolic reasoning over discrete structures such as logical dependencies, causal relationships, and geometric invariants. These principles drive advancements in LLM efficiency, trust, planning and reasoning; as well as generative vision innovations. His research is gratefully supported by NSF, DARPA, ARL, ARO, IARPA, DOE, as well as dozens of industry and university grants. Prof. Wang co-founded the new <a href="https://cpal.cc/">Conference on Parsimony and Learning (CPAL)</a> and served as its inaugural Program Chair. He regularly serves as (senior) area chairs, invited speakers, tutorial/workshop organizers, various panelist positions and reviewers. He is an ACM Distinguished Speaker and an IEEE senior member.</p>

                        <p>Prof. Wang has received many research awards, including an NSF CAREER Award, an ARO Young Investigator Award, an IEEE AI's 10 To Watch Award, an AI 100 Top Thought Leader Award, an INNS Aharon Katzir Young Investigator Award, a Google Research Scholar award, an IBM Faculty Research Award, a J. P. Morgan Faculty Research Award, an Amazon Research Award, an Adobe Data Science Research Award, a Meta Reality Labs Research Award, and two Google TensorFlow Model Garden Awards. His team has won the Best Paper Award at the inaugural Learning on Graphs (LoG) Conference 2022, the Best Paper Finalist Award at the International Conference on Very Large Databases (VLDB) 2024, and five research competition prizes at CVPR/ICCV/ECCV since 2018. He feels most proud of being surrounded by some of the world's most brilliant students: his Ph.D. students include winners of eight prestigious fellowships (NSF GRFP, Apple, NVIDIA, Adobe, IBM, Amazon, Qualcomm, and Snap), among many other honors.</p>
                    </div>
                </div>



                <div class="section-title" style="margin-bottom:20px">
                    <h2>About Our Research</h2>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">


                     <p>At the VITA group, we pursue cutting-edge research spanning the theoretical foundations to practical applications of machine learning (ML). Our group's research continues to evolve, embracing new challenges at the forefront of AI and ML. We collaborate closely with industry partners and other academic institutions to ensure our work has real-world impact and addresses pressing technological needs.</p>

                     <p>Our current work is organized around three key themes, throughout which we maintain a commitment to developing ML algorithms that are efficient, scalable, and robust. We also explore the broader implications of our work, including applications in robotics, healthcare, and AI for social good.</p>

                    </ul>

                        <h4>Theme 1: Large Language Models (LLMs) - Fundamental Optimization, Mechanistic Understanding, & System Co-design</h4>
                        <p>We focus on advancing the efficiency, scalability and trust of LLMs through innovative approaches to training and inference. Our research explores memory-efficient LLM training techniques (<a href="https://arxiv.org/abs/2403.03507">GaLoRe</a> & <a href="https://openreview.net/forum?id=cDYRS5iZ16f">LiGO</a>), efficient generative inference methods (<a href="https://arxiv.org/abs//2306.14048">H<sub>2</sub>O</a> & <a href="https://openreview.net/pdf?id=9vKRhnflAs">Flextron</a>), understanding pre-trained model weights (<a href="https://arxiv.org/abs/2306.03805">essential sparsity</a> & <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf">lottery ticket</a>) or training artifacts (<a href="https://openreview.net/forum?id=O476oWmiNNp">oversmoothening</a> & <a href="https://openreview.net/forum?id=O476oWmiNNp">LLM-PBE</a>): many accompanied with system or hardware co-design.
                        </p>

                        <b style="color:rgb(68, 68, 68)"><i>Selected Notable Works:</i></b>
                        <li>J. Zhao, Z. Zhang*, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian, <b style="color:rgb(71, 71, 71)">"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection”</b>, International Conference on Machine Learning (ICML), 2024. (Oral) <a href="https://arxiv.org/abs/2403.03507">[Paper]</a> <a href="https://github.com/jiaweizzhao/GaLore">[Code] </a>  <em>(Integrated into HuggingFace, PyTorch, LLaMA-Factory, FedML, Axolotl, etc.)</em></li>
                        <li>R. Cai*, S. Muralidharan, G. Heinrich, H. Yin, Z. Wang, J. Kautz, and P. Molchanov, <b style="color:rgb(71, 71, 71)">“Flextron: Many-in-One Flexible Large Language Model”</b>, International Conference on Machine Learning (ICML), 2024. (Oral) <a href="https://openreview.net/pdf?id=9vKRhnflAs">[Paper]</a> <a href="">[Code] </a> </li>
                        <li>Q. Li, J. Hong*, C. Xie, J. Tan, R. Xin, J. Hou, X. Yin, Z. Wang, D. Hendrycks, Z. Wang, B. Li, B. He, and D. Song, <b style="color:rgb(71, 71, 71)">“LLM-PBE: Assessing Data Privacy in Large Language Models”</b>, International Conference on Very Large Data Bases (VLDB), 2024. (Best Paper Finalist)  <a href="https://www.vldb.org/pvldb/vol17/p3201-li.pdf">[Paper]</a> <a href="https://llm-pbe.github.io/home">[Code] </a> </li>
                        <li>Z. Zhang*, Y. Sheng, T. Zhou, T. Chen*, L. Zheng, R. Cai*, Z. Song, Y. Tian, C. Ré, C. Barrett, Z. Wang, and B. Chen, <b style="color:rgb(71, 71, 71)">"H<sub>2</sub>O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2023. <a href="https://arxiv.org/abs//2306.14048">[Paper]</a> <a href="https://github.com/FMInference/H2O">[Code] </a> <em>(Integrated into DeepSpeed by Microsoft, and Llama-Recipes by Meta)</em></li> 
                        <li>A. Jaiswal*, S. Liu*, T. Chen*, Z Wang, <b style="color:rgb(71, 71, 71)">"The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2023. <a href="https://arxiv.org/abs/2306.03805">[Paper]</a> <a href="https://github.com/VITA-Group/essential_sparsity">[Code] </a> </li>
                        <li>P. Wang*, R. Panda, L. Hennigen, P. Greengard, L. Karlinsky, R. Feris, D. Cox, Z. Wang, and Y. Kim, <b style="color:rgb(71, 71, 71)">"Learning to Grow Pretrained Models for Efficient Transformer Training”</b>, International Conference on Learning Representations (ICLR), 2023. (Spotlight) <a href="https://openreview.net/forum?id=cDYRS5iZ16f">[Paper]</a> <a href="https://github.com/VITA-Group/LiGO">[Code] </a> <em> (Implemented in IBM’s AI production system)</em></li> 
                        <li>P. Wang*, W. Zheng*, T. Chen*, and Z. Wang, <b style="color:rgb(71, 71, 71)">“Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice”</b>, International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=O476oWmiNNp">[Paper]</a> <a href="https://github.com/VITA-Group/ViT-Anti-Oversmoothing">[Code]</a></li> 
                        <li>T. Chen*, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin, <b style="color:rgb(71, 71, 71)">“The Lottery Ticket Hypothesis for Pre-trained BERT Networks”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/BERT-Tickets">[Code]</a></li>
                         <br>



                        <h4>Theme 2: Optimization in Modern ML - Learning to Optimize, Black-box optimization, Neurosymbolic learning </h4>
                        <p>
                           Our research in this theme focuses on developing novel optimization techniques for modern machine learning challenges. We have spearheaded the Learning to Optimize (L2O) framework (<a href="http://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds">LISTA-CPSS</a> & <a href="https://openreview.net/forum?id=B1lnzn0ctQ">ALISTA</a>) and benchmark (<a href="https://jmlr.org/papers/v23/21-0308.html">L2O Primer</a>), and recently explore the new frontiers in black-box LLM optimization (<a href="https://openreview.net/forum?id=Ifz3IgsEPX">DP-OPT</a>) and neurosymbolic AI (<a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf">formal fine-tuning</a>, <a href="https://openreview.net/forum?id=ef0nInZHKIC">symbolic L2O</a>, <a href="https://ieeexplore.ieee.org/abstract/document/10694733">neurosymbolic visual RL</a>, & <a href="https://arxiv.org/abs/2411.01639">neurosymbolic uncertainty</a>). 
                        </p>

                        <b style="color:rgb(68, 68, 68)"><i>Selected Notable Works:</i></b>
                        <li>N. Bhatt*, Y. Yang, R. Siva, D. Milan, U. Topcu, and Z. Wang, <b style="color:rgb(71, 71, 71)">"Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework”</b>, arXiv preprint arXiv:2411.01639, 2024. <a href="https://arxiv.org/abs/2411.01639">[Paper]</a> <a href="https://anonymous.4open.science/r/Disentangle-Uncertainty-F476/README.md">[Code] </a> </li>
                        <li>W. Zheng*, S. Sharan*, Z. Fan*, K. Wang*, Y. Xi*, and Z. Wang, <b style="color:rgb(71, 71, 71)">“Symbolic Visual Reinforcement Learning: A Scalable Framework with Object-Level Abstraction and Differentiable Expression Search”</b>, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024. <a href="https://ieeexplore.ieee.org/abstract/document/10694733">[Paper]</a> <a href="https://github.com/VITA-Group/DiffSES">[Code]</a></li>
                        <li>Y. Yang, N. Bhatt*, T. Ingebrand, W. Ward, S. Carr, Z. Wang, and U. Topcu, <b style="color:rgb(71, 71, 71)">"Fine-Tuning Language Models Using Formal Methods Feedback”</b>, Conference on Machine Learning and Systems (MLSys), 2024. <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf">[Paper]</a> <a href="">[Code] </a> </li>
                        <li>J. Hong*, J. Wang, C. Zhang, Z. LI*, B. Li, and Z. Wang, <b style="color:rgb(71, 71, 71)">"DP-OPT: Make Large Language Model Your Differentially-Private Prompt Engineer”</b>, International Conference on Learning Representations (ICLR), 2024. (Spotlight) <a href="https://openreview.net/forum?id=Ifz3IgsEPX">[Paper]</a> <a href="https://github.com/VITA-Group/DP-OPT">[Code] </a> </li>
                        <li>(α-β) T. Chen*, X. Chen*, W. Chen*, H. Heaton, J. Liu, Z. Wang, and W. Yin, <b style="color:rgb(71, 71, 71)">“Learning to Optimize: A Primer and A Benchmark”</b>, Journal of Machine Learning Research (JMLR), 2022. <a href="https://jmlr.org/papers/v23/21-0308.html">[Paper]</a> <a href="https://github.com/VITA-Group/Open-L2O">[Code]</a></li> 
                        <li>W. Zheng*, T. Chen*, T. Hu*, and Z. Wang, <b style="color:rgb(71, 71, 71)">“Symbolic Learning to Optimize: Towards Interpretability and Scalability”</b>, International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=ef0nInZHKIC">[Paper]</a> <a href="https://github.com/VITA-Group/Symbolic-Learning-To-Optimize">[Code]</a></li>
                        <li>J. Liu, X. Chen*, Z. Wang, and W. Yin, <b style="color:rgb(71, 71, 71)">“ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA”</b>, International Conference on Learning Representations (ICLR), 2019. <a href="https://openreview.net/forum?id=B1lnzn0ctQ">[Paper]</a> <a href="https://github.com/TAMU-VITA/ALISTA">[Code]</a></li>
                        <li>X. Chen*, J. Liu, Z. Wang, and W. Yin, <b style="color:rgb(71, 71, 71)">“Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2018. (Spotlight) <a href="http://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds">[Paper]</a> <a href="https://github.com/TAMU-VITA/LISTA-CPSS">[Code]</a></li>
                         <br>



                        <h4>Theme 3: Generative Vision - 3D/4D/Video Synthesis, and Related Applications</h4>
                        <p>
                            Our group's earlier (pre-2021) work includes several influential algorithms for GAN-based image enhancement and editing “in the wild”. More recently (post-2021), we push the boundaries of generative AI for visual tasks, with a focus on 3D/4D reconstruction (<a href="https://largespatialmodel.github.io/">LSM</a>, <a href="https://arxiv.org/abs/2403.20309">InstantSplat</a>, <a href="https://arxiv.org/abs/2311.17245">LightGaussian</a>, & <a href="https://arxiv.org/abs/2211.16431">NeuralLift-360</a>), novel view synthesis (<a href="https://openreview.net/forum?id=xE-LtsE-xx">GNT</a> & <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136820712.pdf">SinNeRF</a>), and video generation (<a href="https://arxiv.org/abs/2403.14773">StreamingT2V</a> & <a href="https://arxiv.org/abs/2303.13439">Text2Video-Zero</a>).  
                        </p>


                        <b style="color:rgb(68, 68, 68)"><i>Selected Notable Works:</i></b>
                        <li>Z. Fan*, J. Zhang, W. Cong*, P. Wang*, R. Li, K. Wen, S. Zhou, A Kadambi, Z. Wang, D. Xu, B. Ivanovic, M. Pavone, and Y. Wang, <b style="color:rgb(71, 71, 71)">“Large Spatial Model: End-to-end Unposed Images to Semantic 3D”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2024. <a href="https://arxiv.org/abs/2410.18956">[Paper]</a> <a href="https://largespatialmodel.github.io/">[Code] </a> 
                        <li>Z. Fan*, K. Wang*, K. Wen, Z. Zhu*, D. Xu*, and Z. Wang, <b style="color:rgb(71, 71, 71)">"LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS”</b>, Advances in Neural Information Processing Systems (NeurIPS), 2024. (Spotlight) <a href="https://arxiv.org/abs/2311.17245">[Paper]</a> <a href="https://github.com/VITA-Group/LightGaussian">[Code] </a> </li> 
                        <li>Z. Fan*, W. Cong*, K. Wen, K. Wang*, J. Zhang, X. Ding, D. Xu, B. Ivanovic, M. Pavone, G. Pavlakos, Z. Wang, and Y. Wang, <b style="color:rgb(71, 71, 71)">"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds”</b>, arXiv preprint arXiv:2403.20309, 2024. <a href="https://arxiv.org/abs/2403.20309">[Paper]</a> <a href="https://instantsplat.github.io/">[Code] </a> </li>
                        <li>R. Henschel, L. Khachatryan, D. Hayrapetyan, H. Poghosyan, Vahram Tadevosyan, V. Tadevosyan,  Z. Wang, S. Navasardyan, and H. Shi, <b style="color:rgb(71, 71, 71)">"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text”</b>, arXiv preprint arXiv:2403.14773, 2024. <a href="https://arxiv.org/abs/2403.14773">[Paper]</a> <a href="https://github.com/Picsart-AI-Research/StreamingT2V">[Code] </a> </li> 
                        <li>L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi, <b style="color:rgb(71, 71, 71)">"Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators”</b>, IEEE International Conference on Computer Vision (ICCV), 2023. (Oral) <a href="https://arxiv.org/abs/2303.13439">[Paper]</a> <a href="https://github.com/Picsart-AI-Research/Text2Video-Zero">[Code] </a> <em> (Commercialized as <a href="https://picsart.com/blog/post/introducing-your-new-favorite-unhinged-ai-tool-ai-gif-generator">Picsart AI GIF generator</a>)</em></li> 
                        <li>D. Xu*, Y. Jiang*, P. Wang*, Z. Fan*, Y. Wang*, and Z. Wang, <b style="color:rgb(71, 71, 71)">"NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360◦ Views”</b>, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. (Highlight) <a href="https://arxiv.org/abs/2211.16431">[Paper]</a> <a href="https://vita-group.github.io/NeuralLift-360/">[Code] </a> </li>
                        <li>M. Varma*, P. Wang*, X. Chen*, T. Chen*, S. Venugopalan, and Z. Wang, <b style="color:rgb(71, 71, 71)">"Is Attention All That NeRF Needs?”</b>, International Conference on Learning Representations (ICLR), 2023. <a href="https://openreview.net/forum?id=xE-LtsE-xx">[Paper]</a> <a href="https://github.com/VITA-Group/GNT">[Code]</a> </li>
                        <li>D. Xu*, Y. Jiang*, P. Wang*, Z. Fan*, H. Shi, and Z. Wang, <b style="color:rgb(71, 71, 71)">“SinNeRF: Training Neural Radiance Field on Complex Scenes from a Single Image”</b>, European Conference on Computer Vision (ECCV), 2022. <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136820712.pdf">[Paper]</a> <a href="https://vita-group.github.io/SinNeRF/">[Code] </a> </li>
                         <br>

                       
                        <!-- <br> -->
                        <h5><a href="./prospective_students.html">Prospective Students Shall Read More...</a></h5>
                    </div>
                </div>




            <div class="section-title" style="padding-top: 20px">
                <h2>Sponsor</h2>
            </div>
            <div class="trend-entry d-flex">
                <div class="row justify-content-md-center">
                    <div>
                        <img src="./123.png" width="100%"/>
                    </div>
                </div>
            </div>
        </div>

        </div>
    </div>

        
<!-- END section -->


<div class="footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="copyright">
                    <p>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        Copyright &copy;<script>document.write(new Date().getFullYear());</script>
                        All rights reserved | Built upon <a
                            href="https://colorlib.com" target="_blank">Colorlib</a>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        </p>
                        
                </div>
            </div>
        </div>
    </div>
</div>

</div>
<!-- .site-wrap -->


<!-- loader -->
<!-- <div id="loader" class="show fullscreen">
    <svg class="circular" width="48px" height="48px">
        <circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/>
        <circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10"
                stroke="#ff5e15"/>
    </svg>
</div> -->

<script src="js/jquery-3.3.1.min.js"></script>
<script src="js/jquery-migrate-3.0.1.min.js"></script>
<script src="js/jquery-ui.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/owl.carousel.min.js"></script>
<script src="js/jquery.stellar.min.js"></script>
<script src="js/jquery.countdown.min.js"></script>
<script src="js/bootstrap-datepicker.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/aos.js"></script>
<script src="js/jquery.fancybox.min.js"></script>
<script src="js/jquery.sticky.js"></script>
<script src="js/jquery.mb.YTPlayer.min.js"></script>

<script src="js/main.js"></script>

</body>

</html>
